{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yizhao-Wang/ChatGPT-s-PPO/blob/main/Understanding_the_PPO_algorithm_behind_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# library"
      ],
      "metadata": {
        "id": "qcxk0XobbCMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhXS4eg3M0Pb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "def sigmoid(x):\n",
        "    \"\"\"Compute the sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "4q9maDbIbLcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview of ChatGPT**\n",
        "Today we are surprised by the performance of ChatGPT in many areas. It can help people do a lot of work.\n",
        "\n",
        "\n",
        "ChatGPT is an AI system that can engage in back-and-forth conversational interactions in a chatbot-style interface. It is capable of writing code, correcting or adjusting its responses based on the user's feedback.\n",
        "ChatGPT is a complex system built on top of a large language model, such as GPT-3.5, and trained using both supervised learning and reinforcement learning methods.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mn5i5oPwN2jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following section we will first introduce the procedure of training ChatGPT, which includes the brief outline about the PPO method. Then we will give concrete explanation of the PPO algorithm and its predecessors - NPO and TRPO."
      ],
      "metadata": {
        "id": "DF2vEBtrkpZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How to train ChatGPT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-zdWNtswkvIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure of training mainly follows the article of [InstructGPT](https://arxiv.org/abs/2203.02155).\n",
        "![picture](https://drive.google.com/uc?export=view&id=17lxcogettj8PwByqrmEqBgeQwA9ooyF8)\n"
      ],
      "metadata": {
        "id": "QRWTVFha-q36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 0: Pre-train GPT-3.5**"
      ],
      "metadata": {
        "id": "BQqZsxEGl5wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3.5 is a large generative language model invented by openAI and trained by a self-supervised method (a self-supervised method is a kind of method whose training data can also be used as labels). There is no official article about it, but we assume that it's basically the extension of GPT-3 with more training data and model parameters.\n",
        "\n",
        "(1) The training data set is constructed from the language data collected from the Internet. Any sentence within dialog, translation and article can be used as training data.\n",
        "\n",
        "(2) As for training, GPT-3.5 uses the decoder part of [Transformer](https://arxiv.org/abs/1706.03762) as its architecture whose point is the masked multi-head attention mechanism. The task is to continuously predict the distribution of the next word and select the most possible one given the previous content until the end of the sentence. The last generated word can be integrated into the previous content as the input to the next word prediction. The masked attention mechanism can ensure that the model generates the output without seeing the real label (the content behind the input), so the prediction can be done simultaneously if we always use the real previous content as input.\n",
        "\n",
        "(3) The original sentence can be the label for training. The distance between the real word and the actual output can be the loss. We train the model iteratively so that it can generate the sentence that is close to the original one."
      ],
      "metadata": {
        "id": "DbNB019Hkwga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The correspondance between GPT and RL\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1iSJWDxaL85iPQ62JOcTZqsfTbN5_Z7Ac)\n",
        "\n",
        "This figure shows the interaction between the actor(agent) and environment. We can construct a mapping from GPT task to RL task. Here, we can see this self-supervised training as **imitation learning** in RL. We directly learn from the label which is the expert action. We hope our model can perform as good as the expert.\n",
        "\n",
        "For example, we want the GPT to generate the answer of \"How are you?\".\n",
        "* Env = GPT input module.\n",
        "* $\\space s_1 \\space$     = 'how are you?'\n",
        "* actor = GPT generate module\n",
        "* $\\space a_1 \\space$ = 'I'\n",
        "* $\\space s_2 \\space$ = 'how are you? I'\n",
        "* $\\space a_2 \\space$ = 'am'\n",
        "* $\\space s_3 \\space$ = 'how are you? I am'\n",
        "* $\\space a_3 \\space$ = 'fine'\n",
        "* $\\space s_4 \\space$ = 'how are you? I am fine'\n",
        "* $\\space a_4 \\space$ = '.'\n",
        "* $\\space s_T \\space$ = 'how are you? I am fine.'\n",
        "\n",
        "GPT input module(Env) keeps combining the previous content($s_{t}$) and new generated word($a_t$), which forms new content($s_{t+1}$). Then, GPT generate module(Actor) keeps generate the new word($a_{t+1}$). The procedure continually goes on until the final word($s_T$) is generated. As for the reward, since we have the true label, the real content itself, we can assign high reward to those correct output while low or even negetive reward to those wrong output.\n",
        "Plus, if we want to construct a conversation model, we can just merge the previous dialogue into the present question and form a new state.\n",
        "\n"
      ],
      "metadata": {
        "id": "mitnRE0pk43P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1: Fine-tuning GPT-3.5**"
      ],
      "metadata": {
        "id": "HdKSCzjql-1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The prompt is usually the question or something else that triggers the conversation that follows. The labeler is a human who is hired to answer the prompt. The answer and prompt can be combined together as the training data mentioned above. The steps follows are just like the procedures to train the GPT-3.5. The most layer in the front of GPT-3.5 might be frozen to prevent overfitting on new data and keep the training stable.\n",
        "\n",
        "As for the reason of this step, it is to benefit the following step.\n",
        "\n",
        "On the one hand, in the next step, we need to rank several outputs that the model generates. We want to get a better model to generate good-enough answers so that the ranking can be made and meaningful. If all sentences have nothing to do with the correct answer, the ranking will not make sense and the model cannot learn from these noisy data. Therefore, this further training can improve the performance of the model.\n",
        "\n",
        "On the other hand, this step is still like imitation learning. The distribution of responses is determined by the export policy, which is the data collected from the Internet before. The distribution of this data may be far from the dialog type data we want. Since we hope to get a dialog LLM here, these carefully designed prompts and answers can help shift the focus of the model from the general aspect to the question-answer orientation."
      ],
      "metadata": {
        "id": "Wr47sn4Uk6Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Train the Reward Model**"
      ],
      "metadata": {
        "id": "ixYPxJrcmCum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The training procedure of the general language model mainly consists of the above steps. Although it can achieve good performance, the answers generated by human are too expensive. If we have a model that can give advice to the answers generated by the language model,\n",
        "we may not need the human's help anymore. However, this assumption assumes that we already have an almost good model that can generate almost correct answers. Fortunately, our fine-tuned GPT-3.5 already meets this condition.\n",
        "\n",
        "\n",
        "Here is the loss function of the reward model:\n",
        "$$\\operatorname{loss}\\left(\\theta\\right)=-\\frac{1}{\\binom{K}{2}}E_{(x,y_w,y_l)\\sim D}\\left[\\log\\left(\\sigma\\left(r_\\theta\\left(x,y_w\\right)-r_\\theta\\left(x,y_l\\right)\\right)\\right)\\right] ≈ \\frac{1}{\\binom{K}{2}}\\sum_{1 \\leq y_l\\leq y_w \\leq K}\\left[\\log\\left(\\sigma\\left(r_\\theta\\left(x,y_w\\right)-r_\\theta\\left(x,y_l\\right)\\right)\\right)\\right]$$\n",
        "K = the number of answers generated by the GPT-3.5. x = prompt. $y_w$ = the answer with the higher rank and $y_l$ = the answer with the lower rank.\n",
        "The rank is graded by the human.\n",
        "With sigmoid function $σ()$, the loss will decrease when we have higher reward $r_θ(x,y_w)$ for better answer $y_w$ and lower reward $r_θ(x,y_l)$ for worse answer $y_l$. Since we have K answers, we can choose $\\binom{K}{2}$ pair of answers to calculate the loss, so we divide $\\binom{K}{2}$ to get the average loss.\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1S6wh7Pn61ZRxt9npa09MW5osPsDNfUmK)\n",
        "\n",
        "The figure above shows an example of the training. K=3, x='how are you?', the best y='I am good.', second y='I am terrible' and the worst y='Good weather.'\n",
        "We hope that the reward model can output the reward according to the ranking by human annotation. It doesn't need to generate the actual ranking number, while it is encouraged to generate the reward with large differences between different answers, like 100 for best y and -100 for worst y.\n",
        "\n",
        "By calculating this loss and backpacking the gradient, we can get the reward model we want and finally get rid of the annotation of human.\n"
      ],
      "metadata": {
        "id": "oHsyXr81k9P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why use RL method to train\n",
        "\n",
        "Before we go to the next step, we want to talk about the importance of using the RL method to train our model.\n",
        "\n",
        "On the one hand, as we talked above, with the help of the reward model that comes from RL, we don't need the expensive human annotation any more.\n",
        "\n",
        "On the other hand, if the model interacts with the human who likes the agent interacting with the environment, things might change over time. We cannot be sure that the model we have trained is 100% correct all the time. We need to make some changes to our policy after we find some problems when interacting with humans. However, we don't want to retrain the whole model, including collecting a new batch of data, fine-tuning and so on, which is too time-consuming and expensive. Therefore, we hope to use the RL method to continuously update our model in the direction of better alignment with the human mind, while saving the cost as much as possible. Just using the prompt went wrong to fine-tuning our reward model until it can judge good answer and bad answer again, we can let the system train itself step by step using the RL method without human assistance."
      ],
      "metadata": {
        "id": "S4mGZg1hlAGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3:Retrain the GPT with PPO algorithm**\n",
        "\n"
      ],
      "metadata": {
        "id": "iE0x2A9HmGzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1U9mdLRwPuurpyus7IpraFDUbGR5ohxZc)\n",
        "\n",
        "The figure above shows the retraining procedure of PPO algorithm.\n",
        "\n",
        "\n",
        "**Input :** The new prompts dataset collected for RL procedure.\n",
        "\n",
        "\n",
        "\n",
        "**Initialization:** Initial Base Language Model and first generation PPO model are the copies of the fine-tuned GPT model in step 1. Reward model is fixed from step 2.\n",
        "\n",
        "**Opejective:** \\begin{aligned}\n",
        "\\mathrm{objective}\\left(\\phi\\right)=& E_{(x,y)\\sim D_{\\pi_{\\phi}^{\\mathrm{PPO}}}}\\left[r_{\\theta}(y|x)-\\lambda_{KL}\\log\\left(\\pi_{\\phi}^{\\mathrm{PPO}}(y\\mid x)/\\pi^{\\mathrm{Base}}(y\\mid x)\\right)\\right]+  \\gamma E_{x\\sim D_{\\mathrm{pretrain}}}\\left[\\log(\\pi_{\\phi}^{\\mathrm{PPO}}(x))\\right]\n",
        "\\end{aligned}\n",
        "\n",
        "*$E_{(x,y)\\sim D_{\\pi_{\\phi}^{\\mathrm{PPO}}}}$ means the expectation of the function in the blackets whose data distribution(x,y)[mainly y] is determined by the $\\phi$(PPO policy). We hope to maximize this expectation so that the PPO policy can generate better answers.\n",
        "\n",
        "*$r_\\theta(y|x)$ is the output of the reward model given the input x(prompt) and output y(answer). This term is fixed by the reward model.\n",
        "\n",
        "*$λ_{KL}$ is the coefficient of the KL penalty.  $\\log(\\pi_{\\phi}^{\\mathrm{PPO}}(y\\mid x)/\\pi^{\\mathrm{Base}}(y\\mid x))$ is the KL penalty to measure the distance between the distribution of PPO policy's answer and base policy's answer. We hope to minimize this term so that the policy does not change dramatically. Many language model training methods also use some penalty term such as L1 norm or L2 norm to constrain the updating of the model.\n",
        "\n",
        "*$γ$ is the coefficient to control the generative performance regressions on other NLP datasets. $E_{x\\sim D_{\\mathrm{pretrain}}}\\left[\\log(\\pi_{\\phi}^{\\mathrm{PPO}}(x))\\right]$ is almost the same objective for step 0 and 1. During the PPO training period, this term is usually set to be 0.\n",
        "\n",
        "\n",
        "**Loop until the objective function stops growing**\n",
        " >feed the prompts into the model\n",
        "\n",
        " >calculate the objective function\n",
        "\n",
        " >update the PPO policy using the gradient acsent method\n",
        "\n",
        " >change the base model with the updated PPO model\n",
        "\n",
        "Actually, the above procedures are only the rough introduction of the non-optimized PPO policy training method. In practice, to avoid constantly updating the base model and to take full advantage of the system-generated answers, we usually change our base model after several updates of the PPO model. In this way, we need to make a small modification to the first term of our objective function. The concrete explanation of the PPO algorithm will be shown in the next part.\n"
      ],
      "metadata": {
        "id": "4lKESNTblEIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Proximal Policy Optimization(PPO) algorithm: Introduction**\n",
        "\n",
        "The PPO algorithm is a policy gradient method for reinforcement learning which has plenty of advantage while being much simpler and also, it is the the default reinforcement learning algorithm at OpenAI. To better understand this core algorithme behind the training of GPT, we will first see its predecssors: NPO and TRPO."
      ],
      "metadata": {
        "id": "gXk84pJDQTFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Natural Policy Optimization**"
      ],
      "metadata": {
        "id": "vCbvvFzsM2J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In policy gradient methods, we use gradient ascent to update the parameter of a policy in the parameter space.$$\n",
        "d^* = \\underset{\\|d\\|\\leq\\epsilon}{\\text{arg max}} \\, J(\\theta + d)\n",
        "$$ However, sometimes this can cause problems, as minor changes in the parameter space could have significant differences in the policy space. Below is an example.\n",
        "\n",
        "Note that a policy is a distribution, π(a∣s), that takes a state and gives the action."
      ],
      "metadata": {
        "id": "7UvXLz3NU9LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of theta values for demonstration\n",
        "theta_values = [-2, 0, 2]\n",
        "\n",
        "# Plot the policy distributions for each theta value\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "for i, theta in enumerate(theta_values):\n",
        "    # Calculate the probabilities for actions given the policy parameter theta\n",
        "    prob_a1 = sigmoid(theta)  # Probability of taking action 1\n",
        "    prob_a2 = 1 - sigmoid(theta)  # Probability of taking action 2\n",
        "\n",
        "    # Plot the probability distribution for the current theta value\n",
        "    axes[i].bar(['a1', 'a2'], [prob_a1, prob_a2], color=['blue', 'orange'])\n",
        "    axes[i].set_title(f\"theta = {theta}\")\n",
        "    axes[i].set_ylim(0, 1)\n",
        "    axes[i].set_ylabel(\"Probability\")\n",
        "    axes[i].set_xlabel(\"Actions\")\n",
        "\n",
        "fig.suptitle('Policy distributions for different values of theta')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "Wp4X2P-lZbRU",
        "outputId": "0422d9d3-9816-47c6-995c-203e1d191e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KL-Divergence"
      ],
      "metadata": {
        "id": "uxCkvn1gM99H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As shown in the above image, we then want to limiting the difference when we do the optimization of parameter. An KL-divergence can be used to constraint that because it mesure how difference between two distributions are. The definition of KL in discret case: $$ D_{KL}(P \\parallel Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "bSQnkbEMht94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the policy distributions for the given theta values\n",
        "theta_values_example = [-2, 0, 2]\n",
        "policy_distributions_example = np.array([[sigmoid(theta), 1 - sigmoid(theta)] for theta in theta_values_example])\n",
        "\n",
        "# Calculate the KL divergences between each pair of these distributions\n",
        "kl_divergences = {}\n",
        "for i, pdist1 in enumerate(policy_distributions_example):\n",
        "    for j, pdist2 in enumerate(policy_distributions_example):\n",
        "        if i < j:  # To avoid redundant calculation and self-comparison\n",
        "            kl_div = entropy(pdist1, pdist2)\n",
        "            kl_divergences[f\"KL(Theta {theta_values_example[i]} || Theta {theta_values_example[j]})\"] = kl_div\n",
        "\n",
        "kl_divergences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_DXZz8Qhrm5",
        "outputId": "37c640a6-fd28-4f49-8763-a6a107867fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'KL(Theta -2 || Theta 0)': 0.32781332547273767,\n",
              " 'KL(Theta -2 || Theta 2)': 1.5231883119115288,\n",
              " 'KL(Theta 0 || Theta 2)': 0.43378083048302674}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###The objective function of Natural Policy Gradient: considering the KL-divergenve as an penalty term\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bonSytAjbw_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to increase the most possible of the policy's parameters but we want to constraint the difference of the updated policy. We then have the objective function :\n",
        "$$\n",
        "d^* = \\underset{d}{\\text{arg max}} \\, J(\\theta + d) \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta}\\parallel\\pi_{\\theta+d}\\right) \\leq \\epsilon\n",
        "$$\n",
        "Convert it to an unconstrained penalized objective :\n",
        "$$\n",
        "d^* = \\underset{d}{\\text{arg max}} \\, J(\\theta + d) - \\lambda \\left( D_{KL}\\left[\\pi_{\\theta} \\parallel \\pi_{\\theta+d}\\right] - \\epsilon \\right)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "Lzy5AZyTNZyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Taylor Expansion of objective and Fisher Information Matrix"
      ],
      "metadata": {
        "id": "h4cC9tRsNLnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The taylor expansion could be used to approximated the objective function:\n",
        "$$\n",
        "\\approx \\underset{d}{\\text{arg max}} \\, J(\\theta_{\\text{old}}) + \\nabla_{\\theta}J(\\theta) |_{\\theta=\\theta_{\\text{old}}} \\cdot d - \\frac{1}{2} \\lambda d^T \\nabla^2_{\\theta} D_{KL}\\left[\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}\\right] |_{\\theta=\\theta_{\\text{old}}} d + \\lambda \\epsilon\n",
        "$$\n",
        "\n",
        "The expansion of kl-divergence penalty term is :$$\n",
        "D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) \\approx D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta_{\\text{old}}}) + d^T \\nabla_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} + \\frac{1}{2} d^T \\nabla^2_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} d\n",
        "$$\n",
        "\n",
        "For the KL penalty term, the first order of expansion is 0, so the second order term is be used.$$\\begin{align*}\n",
        "\\nabla_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} &= - \\nabla_{\\theta} \\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\log P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= - \\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\nabla_{\\theta} \\log P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= - \\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\frac{1}{P_{\\theta_{\\text{old}}}(x)} \\nabla_{\\theta} P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= \\int_{x} P_{\\theta_{\\text{old}}}(x) \\frac{1}{P_{\\theta_{\\text{old}}}(x)} \\nabla_{\\theta} P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= \\int_{x} \\nabla_{\\theta} P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= \\nabla_{\\theta} \\int_{x} P_{\\theta}(x) |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= 0\n",
        "\\end{align*}\n",
        "$$\n",
        "The seconde order can be derived to: $$\n",
        "\\begin{align*}\n",
        "\\nabla^2_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} &= -\\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ \\nabla^2_{\\theta} \\log P_{\\theta}(x) \\right] |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= -\\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ \\nabla_{\\theta} \\left( \\frac{\\nabla_{\\theta} P_{\\theta}(x)}{P_{\\theta}(x)} \\right) \\right] |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= -\\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\nabla^2_{\\theta} P_{\\theta}(x) P_{\\theta}(x) - \\nabla_{\\theta} P_{\\theta}(x) \\nabla_{\\theta} P_{\\theta}(x)^T}{P_{\\theta}(x)^2} \\right] |_{\\theta=\\theta_{\\text{old}}} \\\\\n",
        "&= \\mathbb{E}_{x\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ \\nabla_{\\theta} \\log P_{\\theta}(x) \\nabla_{\\theta} \\log P_{\\theta}(x)^T \\right] |_{\\theta=\\theta_{\\text{old}}}\\\\&=F(\\theta_{\\text{old}})\n",
        "\\end{align*}\n",
        "$$where F is exatly the same as Fisher Information Matrix.\n",
        "So now, \\begin{align*}\n",
        "D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) &\\approx D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta_{\\text{old}}}) + d^T \\nabla_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} + \\frac{1}{2} d^T \\nabla^2_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} d \\\\\n",
        "&= \\frac{1}{2} d^T F(\\theta_{\\text{old}}) d \\\\\n",
        "&= \\frac{1}{2} (\\theta - \\theta_{\\text{old}})^T F(\\theta_{\\text{old}})(\\theta - \\theta_{\\text{old}})\n",
        "\\end{align*}\n",
        "\n",
        "The objective funtion can wirten as minorize problem $$\n",
        "\\begin{align*}\n",
        "d^* &= \\underset{d}{\\text{arg max}} \\, J(\\theta + d) - \\lambda(D_{KL}[\\pi_{\\theta} \\parallel \\pi_{\\theta+d}] - \\epsilon) \\\\\n",
        "&\\approx \\underset{d}{\\text{arg max}} \\, J(\\theta_{\\text{old}}) + \\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}} \\cdot d - \\frac{1}{2} \\lambda (d^T \\nabla^2_{\\theta} D_{KL}[\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}] |_{\\theta=\\theta_{\\text{old}}} d) + \\lambda \\epsilon \\\\\n",
        "&= \\underset{d}{\\text{arg max}} \\, \\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}} \\cdot d - \\frac{1}{2} \\lambda (d^T F(\\theta_{\\text{old}}) d) \\\\\n",
        "&= \\underset{d}{\\text{arg min}} \\, -\\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}} \\cdot d + \\frac{1}{2} \\lambda (d^T F(\\theta_{\\text{old}}) d)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We can set the gradient to zero, $$\\begin{align*}\n",
        "0 &= \\frac{\\partial}{\\partial d} \\left( -\\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}} \\cdot d + \\frac{1}{2} \\lambda (d^T F(\\theta_{\\text{old}})d) \\right) \\\\\n",
        "&= - \\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}} + \\frac{1}{2} \\lambda (F(\\theta_{\\text{old}})d)\n",
        "\\end{align*}\n",
        "$$\n",
        "so that the d which minimize the equation is $$\n",
        "d = \\frac{1}{\\lambda} F^{-1}(\\theta_{\\text{old}}) \\nabla_{\\theta} J(\\theta) |_{\\theta=\\theta_{\\text{old}}}\n",
        "$$ We have though the natual gradient $\\begin{equation}\n",
        "g_N =  F^{-1}(\\theta_{\\text{old}}) \\nabla_{\\theta} J(\\theta) |_{\\theta=\\theta_{\\text{old}}}\n",
        "\\end{equation}\n",
        "$ Note that the $\\frac{1}{\\lambda}$ is an constant. We remplace the $g_N$ into the KL-divergence which is smaller than $\\epsilon$: $$\\begin{align*}\n",
        "D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) &\\approx \\frac{1}{2}(\\theta - \\theta_{\\text{old}})^T F(\\theta_{\\text{old}})(\\theta - \\theta_{\\text{old}}) \\\\\n",
        "&= \\frac{1}{2}(\\alpha g_N)^T F(\\alpha g_N) = \\epsilon\n",
        "\\end{align*}\n",
        "$$Finally, the step size of the natural gradient is $$\n",
        "\\alpha = \\sqrt{\\frac{2\\epsilon}{g_N^T F g_N}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "t9MERgYaNH0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trust Region Policy Optimization (TRPO)**\n"
      ],
      "metadata": {
        "id": "ehsIzGRz5A6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Motivation: Problems with Natural policy optimization**\n"
      ],
      "metadata": {
        "id": "x5Ks51nJIlIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the motivation behind the TRPO algorithm, let's first examine the limitations of Natural Policy Optimization (NPO). Although the natural gradient $ g_N =  F^{-1}(\\theta_{\\text{old}}) \\nabla_{\\theta} J(\\theta) |_{\\theta=\\theta_{\\text{old}}}$ provides a better direction for parameter update, several issues could be identified:\n",
        "1. The KL divergence constraint may be violated. Since The step size $\\alpha$ is computed using the second-order Taylor expansion approximation of the KL divergence:\n",
        "$$\\begin{align*}\n",
        "D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) &\\approx \\frac{1}{2}(\\theta - \\theta_{\\text{old}})^T F(\\theta_{\\text{old}})(\\theta - \\theta_{\\text{old}}) \\\\\n",
        "&= \\frac{1}{2}(\\alpha g_N)^T F(\\alpha g_N) = \\epsilon\n",
        "\\end{align*}$$\n",
        "there is risk that our step size is too large, causing the KL divergence to exceed the threshold $ \\epsilon $.\n",
        "2. There is no guarantee that  $J(\\theta + d) > J(\\theta)$. To understand this, note that the objective established by NPO is\n",
        "$$\n",
        "d^* = \\underset{d}{\\text{arg max}} \\, J(\\theta + d) \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta}\\parallel\\pi_{\\theta+d}\\right) \\leq \\epsilon\n",
        "$$. This allows us to find the best possible update d, but even the best update can not ensure policy improvement: the new policy $\\pi_{\\theta + d}$ could be worse than the original policy $\\pi_{\\theta}$.\n",
        "3.The search direction is computed by approximately solving the equation $Ax=g$, where $A$ is the Fisher information matrix $F(\\theta_{\\text{old}})$, namely this equation:\n",
        "$$\n",
        "  \\frac{1}{2} \\lambda F(\\theta_{\\text{old}})  d = \\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}}\n",
        "$$\n",
        "where $F(\\theta_{\\text{old}}) =\\nabla^2_{\\theta} D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta}) |_{\\theta=\\theta_{\\text{old}}} $. In large-scale problems, it is prohibitively costly (with respect to computation and memory) to form the full matrix $F\\left(\\mathrm{or}F^{-1}\\right)$.\n",
        "4. Only 1 step of parameter update could be done using rollout episodes generated by $\\pi_{\\theta_{\\text{old}}}$. To accelerate training, it's crucial to use the rollout data more efficiently"
      ],
      "metadata": {
        "id": "Vocj0SV7I88X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are the contributions of TRPO?**\n"
      ],
      "metadata": {
        "id": "8uvvVwP0KQpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, TRPO's contributions are in 4 folds:\n",
        "1. The TRPO paper provides, by viewing the policy gradient processus as a classical policy iteration processus, a theoretical justification for our intuition in Natural Policy Optimization that we should introduce KL divergence constraint in addition to the original optimization objective $\\underset{d}{\\text{arg max}} \\, J(\\theta_{old} + d) $.\n",
        "2. To resolve the first and second issue we mentioned above in the motivation part, i.e. (1) violation of KL constraint and (2) lack of guarantee of really improving the policy, TRPO employs a line search to determine the step size $\\alpha$ such that both (1) make $J(\\theta_{old} + \\alpha \\times  F^{-1}(\\theta_{\\text{old}})\\nabla_{\\theta}J(\\theta) |_{\\theta=\\theta_{\\text{old}}}) > J(\\theta_{old})$ and (2) $D_{KL}(\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_{\\theta_{old} + \\alpha \\times  F^{-1}(\\theta_{\\text{old}})\\nabla_{\\theta}J(\\theta) |_{\\theta=\\theta_{\\text{old}}}}) <$  the threshold we define.\n",
        "3. TRPO utilizes the conjugate gradient algorithm allowing us to approximately solve the equation\n",
        "$$\n",
        "  \\frac{1}{2} \\lambda F(\\theta_{\\text{old}})  d = \\nabla_{\\theta}J(\\theta)|_{\\theta=\\theta_{\\text{old}}}\n",
        "$$\n",
        "without forming the full matrix $F$.\n",
        "4. TRPO introduces importance sampling which allows for multiple steps of parameter updates to use rollout episodes of the previous policy, instead of just one-step parameter update per episode. (N.B. it is still considered as a on-policy algorithm.)\n",
        "\n",
        "In the subsequent subsections, we provide detailed illustrations of these four aspects. Some sections include comprehensive mathematical derivations, which are essential for the technical understanding of TRPO methodology. Readers primarily interested in the overall framework may keep these subsections folded for a more smooth reading experience."
      ],
      "metadata": {
        "id": "Oqp-9sEIhQPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Before all**"
      ],
      "metadata": {
        "id": "8sT44tqgT-GP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that in the previous section on NPO, we intuitively obtain the objective\n",
        "$$\n",
        " \\underset{d}{\\text{arg max}} \\, J(\\theta_{old} + d) \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta_{old}+d}\\right) \\leq \\epsilon\n",
        "$$\n",
        "Now if we think about what J is concretly, it turns out that J is exactly the policy gradient objective, that is, the expected reward approximated by sample reward average of rollout $\\tau$ using $\\pi_{\\theta_{old}}$ ,i.e.\n",
        "$$\n",
        "J(\\theta_{old} + d) = J(\\theta) =\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)] = \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\hat{A}_{\\theta_{old}}(s_t^n,a_t^n)\n",
        "$$\n",
        "where $N$ is the number of episodes of rollout $\\tau$ and t refers to timesteps of each episode. Now we can rewrite our objective as\n",
        "$$\n",
        " \\underset{\\theta}{\\text{max}} \\, \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)] ⇔ \\underset{\\theta}{\\text{max}} \\,\\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\hat{A}_{\\theta_{old}}(s_t^n,a_t^n)  \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta}\\right) \\leq \\epsilon.\n",
        "$$\n",
        "\n",
        "If we compare it with the objective function obtained through theoretical derivation in TRPO, given by:\n",
        "\n",
        "$$\n",
        " \\underset{\\theta}{\\text{max}} \\,\n",
        " \\mathbb{E}_{\\tau\\sim\\pi_{\\theta_{old}}}[\\frac{p_{\\pi_{\\theta}}(\\tau)}{p_{\\pi_{\\theta_{old}}}(\\tau)}R(\\tau)]\n",
        " ⇔\n",
        " \\underset{\\theta}{\\text{max}} \\, \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        "  \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta}\\right) \\leq \\epsilon.\n",
        "$$\n",
        "\n",
        " we will find that the only difference between them is that TRPO introduces important sampling so that as if rollout $\\tau$ is generated by $\\pi_{\\theta}$ (whatever which ) instead of by $\\pi_{\\theta_{old}}$\n",
        "\n",
        "Let us first tackle the importance sampling part."
      ],
      "metadata": {
        "id": "JZY_cggnLhHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importance Sampling**\n"
      ],
      "metadata": {
        "id": "3rQ9tPgWUI5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the NPO objective function, we want to optimize $\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)]$, however episodes $\\tau$ are actually generated by the original policy $\\pi_{\\theta_{old}}$, so it may be innacurate to use $\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}$. In policy Gradient, it is ok because every time we get episodes from a policy $\\pi_{\\theta_{old}}$, we only use them to perform one-step gradient ascent, in this case $\\pi_{\\theta_{old}}$ and $\\pi_{\\theta}$ is closed so it is tolerable think that $\\tau$ is generated by $\\pi_{\\theta}$ although it is actually generated by $\\pi_{\\theta_{old}}$. However, if we would like to take advantage of the same rollout data to perfrom several furthur parameter updates, the difference between $\\pi_{\\theta_{old}}$ and $\\pi_{\\theta}$ could not be ignored and we would be forced to use $\\mathbb{E}_{\\tau\\sim\\pi_{\\theta_{old}}}$. That is why importance sampling comes into play.\n",
        "$$\n",
        "\\begin{align*}\n",
        " \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)] &= ∫_{\\tau} p_{\\pi_{\\theta}(\\tau)}R(\\tau) d\\tau = ∫_{\\tau} p_{\\pi_{\\theta_{old}}}(\\tau) \\frac{p_{\\pi_{\\theta}(\\tau)}}{p_{\\pi_{\\theta_{old}}}(\\tau)}R(\\tau) d\\tau \\\\\n",
        " &= \\mathbb{E}_{\\tau\\sim\\pi_{\\theta_{old}}}[\\frac{p_{\\pi_{\\theta}(\\tau)}}{p_{\\pi_{\\theta_{old}}}(\\tau)}R(\\tau)]\\\\\n",
        "  & = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta_{old}}}[ \\frac{p(s_1)\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)}{p(s_1)\\prod_{t=1}^T\\pi_{\\theta_{old}}(a_t|s_t)p(s_{t+1}|s_t,a_t)}R(\\tau)]\\\\\n",
        "  & = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta_{old}}}[ \\frac{\\prod_{t=1}^T \\pi_\\theta(a_t|s_t)}{\\prod_{t=1}^T\\pi_{\\theta_{old}}(a_t|s_t)}R(\\tau)]\\\\\n",
        "  & = \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        "\\end{align*}\n",
        "$$\n",
        "The benifit of adopting importance sampling is in that we can leverage more efficiently data from rollouts. Specifically, as shown in the following image, a RL algorithm with importance sampling is capable of executing several parameter updates (for example, 5 times in our illustration) using the same rollout data from a previous policy. In large-scale problem, a rollout could be expensive and such property is valuable. On the other hand, an algorithm without importance sampling could only effectuate one parameter update per rollout.\n",
        "\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215155102.png)"
      ],
      "metadata": {
        "id": "pW7JPX9DVQoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we present how TRPO paper derive the above objective by guidance of Policy iteration."
      ],
      "metadata": {
        "id": "dpkgD-iVea9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Theoretical justification for introducing KL divergence**"
      ],
      "metadata": {
        "id": "cKZV4dkYfpr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that for Both NPO and TRPO, we want to optimize $J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)]$. Now let us depart from this and follow the  the derivation of TRPO paper to transform this simple objective function to  the final objective\n",
        "$$\n",
        "\\underset{\\theta}{\\text{max}} \\, \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        " \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta}\\right) \\leq \\epsilon.\n",
        "$$\n",
        "\n",
        "First, denote $J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)]$ as $\\eta(\\pi)$ .\n",
        "\n",
        "TRPO tries to relate this expected discounted return miximizaiton problem to the classical policy iteration problem. Recall that in Policy iteration, we have 2 stages:\n",
        "1. policy evaluation: evaluate the value function $V_{\\pi_{\\theta_{old}}}(S)$ after obtaining a policy $\\pi_{\\theta_{old}}$ from the previous policy improvement step.\n",
        "2. policy improvement: find a new policy by letting for each state s, $\\pi_{\\theta}(s) = \\underset{\\pi_{\\theta}}{\\text{arg max}} \\sum_{s^{\\prime},r}p(s^{\\prime},r|s,a)\\big[r+\\gamma V_{\\pi_{\\theta_{old}}}(s^{\\prime})\\big]$\n",
        "\n",
        "We have theoretical guarantee that,  through the above defined policy improvement method, the new policy chosen is 100% not worse than the previous one.\n",
        "\n",
        "#### Now, by analogy, What about thinking  of $\\eta(\\pi)$ also as a **value function of $\\pi$**? This idea is not coming from nowhere. If we review REINFORCE algorithm from this perspective, we could also split it into 2 stages.\n",
        "\n",
        "1. policy evaluation: **evaluate the value function $\\eta(\\pi_{old})$** by rollout data generated using a policy $\\pi_{\\theta_{old}}$ from the previous policy improvement step  (**in this case we do not have to explicitly evaluate it: just calculating advantage function $A(s,a)$ would provide sufficient information for the policy improvement stage**).  \n",
        "2. policy improvement: obtain $\\pi_{\\theta}$ after gradient ascent using advantage function $A(s,a)$ aiming at maximizing $\\eta(\\pi_{old})$.\n",
        "\n",
        "#### Now, by analogy again, in classical Policy iteration, the policy improvement can 100% find a better policy, Then in our \"policy iteration\" algorithm where $\\eta(\\pi)$ is value function, can we also find a policy improvement method that can promise us to have a better algorithm (rather than gradient ascent which cannot)? The answer by the TRPO paper is **YES**! They state the following policy improvement formula (which can guarantee that the true objective $\\eta$ is non-decreasing):\n",
        "\n",
        "$$\n",
        "\\mathbf{\\pi_{\\theta} = \\underset{\\pi_{\\theta}}{\\text{arg max}}} [L_{\\pi_{old}}(\\pi_{\\theta})-CD_{\\mathrm{KL}}^{\\mathrm{max}}(\\pi_{old},\\pi_{\\theta})]\n",
        "$$\n",
        "\n",
        "where\n",
        "$$\n",
        "\\begin{align*}\n",
        " &D_\\mathrm{KL}^\\mathrm{max}(\\pi,\\tilde{\\pi})=\\operatorname*{max}_sD_{KL}(\\pi(\\cdot|s)\\parallel\\tilde{\\pi}(\\cdot|s))\\\\\n",
        " &{L}_{\\pi_{old}}(\\pi)=\\eta(\\pi_{old})+\\sum_{s}\\rho_{\\pi_{old}}(s)\\sum_{a}\\pi(a|s)A_{\\pi_{old}}(s,a)\\\\\n",
        " &C = \\frac{4 \\epsilon \\gamma }{(1-\\gamma)^{2}}\\\\\n",
        " & \\epsilon = \\operatorname*{max}_{s,a} |A_{\\pi}(s,a)|\\\\\n",
        " & \\text{and $\\eta$ is the discounted rate.}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "This trian of thought could be illustrated as:\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215155012.png)\n",
        "\n",
        "This is called **Monotonic Improvement Guarantee** in TRPO paper. Here we will not bother to prove it, and we could safely ignore details such as what does $\\rho$ mean and how we can come up with this for the sake of coherence. In the following part, we will see how we can get to the final objective by several approximations.\n",
        "\n",
        "**Approximation1**  \n",
        "now we know that by performing the following maximization, we are guaranteed to improve the true objective $\\eta$:\n",
        "\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} \\left[{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)-C{ D}_{\\mathrm{KL}}^{\\mathrm{max}}\\left(\\theta_{\\mathrm{old}},\\theta\\right)\\right].\n",
        "$$\n",
        "In practice, if we used the penalty coefficient C recommended by the theory above, the step sizes would be very small. One way to take larger steps in a robust way is to use a constraint on the KL divergence between the new policy\n",
        "and the old policy, i.e., a **trust region constraint**:\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} [{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)] \\quad s.t.\\quad { D}_{\\mathrm{KL}}^{\\mathrm{max}}\\left(\\theta_{\\mathrm{old}},\\theta\\right) < \\delta\n",
        "$$\n",
        "So here we clearly see that the KL divergence constraint is introduced with theorectical justification. And also the name \"**Trust Region** Policy Optimization\".\n",
        "\n",
        "**Approximation 2**\n",
        "\n",
        "Since for a continuous State space,\n",
        "$$\n",
        "D_\\mathrm{KL}^\\mathrm{max}(\\pi,\\tilde{\\pi})=\\operatorname*{max}_sD_{KL}(\\pi(\\cdot|s)\\parallel\\tilde{\\pi}(\\cdot|s))\n",
        "$$\n",
        "is hard to calculate because we cannot iterate over a non-finite space, we turn to calculate the average KL divergence:\n",
        "$$\n",
        "\\bar{D}_\\mathrm{KL}(\\theta_1 || \\theta_2 ) = \\mathbb{E}_{s \\sim \\rho}[D_{KL}(\\pi(\\cdot|s)\\parallel\\tilde{\\pi}(\\cdot|s))]\n",
        "$$\n",
        "Here, we do not care about the distrbution of s, because we will employ Monte Carlo Sampling (i.e. rollout) to take sample average. Now the objective becomes:\n",
        "\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} [{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)] \\quad s.t.\\quad  {\\bar{D}_{\\mathrm{KL}}}\\left(\\theta_{\\mathrm{old}},\\theta\\right) < \\delta\n",
        "$$\n",
        "\n",
        "Lastly it turns out that (details could be seen in the paper, just pure calculation)\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} [{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)]  \\text{ is equivalent to }\n",
        "$$\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        "$$\n",
        "\n",
        "if we introduce importance sampling. Now we get the afore-mentioned TRPO objective function:\n",
        "$$\n",
        "\\underset{\\theta}{\\text{max}} \\, \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        " \\quad \\text{s.t.} \\quad KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta}\\right) \\leq \\delta.\n",
        "$$\n",
        "\n",
        "Until now, we have explored the evolution of TRPO from its origins in policy iteration algorithms, detailing its step-by-step introduction of the KL divergence constraint and showing that the same objective function with Natural Policy Optimization is finally obtained. Although several approximations are made so that the theoretical guarantee are not anymore strictly valid, parctical experiments show that it works robustly."
      ],
      "metadata": {
        "id": "nTJrex2ofEie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Line search**"
      ],
      "metadata": {
        "id": "gHxAOjGDKwZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have shown that TRPO and NPO share the same objective function, it is natural that TRPO would use the NPO parameter updates direction\n",
        " $$\n",
        "g_k =  F^{-1}(\\theta_{\\text{old}}) \\nabla_{\\theta} J(\\theta) |_{\\theta=\\theta_{\\text{old}}}\n",
        "$$\n",
        "as well as the step size\n",
        "$$\n",
        "\\alpha = \\sqrt{\\frac{2\\delta}{g_k^T F g_k}}\n",
        "$$\n",
        "However, in addition, TRPO applies a line search to really ensure improvement of $L_{\\theta_{old}}(\\theta)$ and satisfaction of the KL divergence constraint. This is reasonable because we indeed did so much approximation that KL divergence constraint is not satisfied. Moreover, from the theoretical derivation of the preivous section, we know that if we can improve $L_{\\theta_{old}}(\\theta)$, we have the \"monotonic improvement guarantee\" to also improve $\\eta$, so the second goal of the line search is to improve $L$. More concretely, here is the pseudo code of the line search:\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215175749.png)\n",
        "\n",
        "In the image, $\\alpha \\in (0,1) $ is a decay rate that exponentially vanishes to 0 with j. We can also illustrate it as\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215182023.png)\n",
        "we may see that by shrinking the original step size determined by NPO, we could avoid from violating constraint.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEgOKcTiKYJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conjugate Gradient**"
      ],
      "metadata": {
        "id": "zwRgcl_0LVCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is one of the main differences from the NPO algorithm. When performing gradient ascent in NPO, it is necessary to compute the inverse of the Fisher information matrix $F(θ)$, denoted as $F(θ)^{-1}$. This requires a very high computational resource. If the policy's parameter has a dimension n = 100, the matrix F contains 10,000 elements. Computing the inverse matrix $F(θ)^{-1}$ using the common method of Gaussian elimination requires $O(n^3)$ steps. As an improvement for simplicity, the conjugate gradient method is used, which only requires less than n steps(O(n) complexity). This means that the conjugate gradient scales linearly with the parameters, whereas in NPO, it scales cubically.\n",
        "In conjugate gradient, we do not need to compute $F^{-1}$, but can directly solve $$x = F^{-1}g$$\n",
        "Solving $Fx = g$ is equivalent to minimizing $$f(x) = \\frac{1}{2}x^TFx - g^Tx$$ since $f'(x) = Fx - g = 0$.\n",
        "By using the Conjugate Gradient method to minimize $f(x)$, we can solve $Fx = g$ without computing the matrix inverse, which is computationally more efficient ($O(n)$).\n",
        "\n",
        "The pseudo code of CG:\n",
        "1. **Initialization**:\n",
        "   - Choose an initial guess $x_0$, usually the zero vector.\n",
        "   - Compute the residual $r_0 = g - Fx_0$.\n",
        "   - Set the search direction $p_0 = r_0$.\n",
        "\n",
        "2. **Iteration**:\n",
        "   For each iteration $k$:\n",
        "   - Compute the step size $\\alpha_k = \\frac{r_k^Tr_k}{p_k^TFp_k}$.\n",
        "   - Update the solution $x_{k+1} = x_k + \\alpha_k p_k$.\n",
        "   - Compute the new residual $r_{k+1} = r_k - \\alpha_k Fp_k$.\n",
        "   - If $r_{k+1}$ is sufficiently small, then stop.\n",
        "   - Compute the coefficient for the next search direction $\\beta_k = \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}$.\n",
        "   - Update the search direction $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n",
        "\n",
        "3. **Termination**:\n",
        "   - The algorithm stops when the residual $r_k$ is sufficiently small, or the maximum number of iterations has been reached.\n",
        "\n"
      ],
      "metadata": {
        "id": "hL8Ow_8DLRmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Proximal Policy Optimization (PPO)**"
      ],
      "metadata": {
        "id": "VDa7mJlLyoUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we comes finally to PPO. The idea of PPO is to be able to scalle up TRPO. As we mentioned before, TRPO uses second-order optimization techniques and that requires calculating Hessian Matrix. In contrast, PPO uses first-order optimization, i.e. SGD, so it is less computation-consuming.\n",
        "\n",
        "To be able to use first-order optimization while ensuring KL divergence, the paper proposed 2 alternatives:\n",
        "\n",
        "1. Adaptive KL Penalty Coefficient. The idea is, recall that the reason why we introduce the KL divergence in a constraint form is that in the original objective\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} \\left[{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)-C{ D}_{\\mathrm{KL}}^{\\mathrm{max}}\\left(\\theta_{\\mathrm{old}},\\theta\\right)\\right].\n",
        "$$\n",
        "the value of C is problematique. Now PPO proposes that apart from the solution of TRPO, there exist another solution to this: make C adaptable. So in the end, the objective of PPO is\n",
        "$$\n",
        "\\underset{\\theta}{\\text{max}} \\, \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n)\n",
        "  - \\mathbb{\\beta} \\times KL\\left(\\pi_{\\theta_{old}}\\parallel\\pi_{\\theta}\\right).\n",
        "$$\n",
        "where $\\beta$ takes the place of $C$ in TRPO. The concret way we adjust the $\\beta$ is:\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215193512.png)\n",
        "Namely, we will first set a target KL-divergence value $d_{targ}$. Then after the parameter updating, if we found that the real KL-divergence is too large, then we augment $\\beta$ to augment penalty. Otherwise, if d is too small, we shrink $\\beta$. Then the updated β is used for the next policy update\n",
        "2. Clipped Surrogate Objective. The idea is, to avoid policy change, instead of introducing KL divergence which augments significantly the computation complexity, we may investigate more straight-away methods. For example, the original TRPO objective without KL is\n",
        "\n",
        "$$\n",
        " [{ L}_{\\theta_{\\mathrm{old}}}\\left(\\theta\\right)]  \\text{ which is equivalent to }\n",
        "$$\n",
        "\n",
        "$$\n",
        " \\frac{1}{N}\\sum_{n=1}^N\\prod_{t=1}^t\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}\\hat{A}(s_t^n,a_t^n) := \\hat{\\mathrm{E}}_{t}\\Bigl[r_{t}(\\theta)\\hat{A}_{t}\\Bigr]\n",
        "$$\n",
        "\n",
        "now consider\n",
        "$$\n",
        "L^{C L I P}(\\theta)=\\mathrm{\\hat{E}}_{t}\\Bigl[\\mathrm{min}(r_{t}(\\theta)\\hat{A}_{t},\\mathrm{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t})\\Bigr]\n",
        "$$\n",
        "Where epsilon is defined by us, for example, 0.2. This modification enables the algorithm to penalize changes to the policy that\n",
        "move $r_t(\\theta)$ away from 1. Too see this, let us first consider the function clip.\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215200539.png)\n",
        "Now if we superpose the function min, it will becomes:\n",
        "![](https://raw.githubusercontent.com/YuchenHui22314/picgo/main/img/20231215195841.png)\n",
        "First, we should realize the meaning of \"clip\". if $\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}$ is clipped, then there will be no gradient signal for this A on the $\\theta$.Here, we see that, when $A > 0$, which means, if the gradient is not 0, then this positive A will still make $\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}$ larger, in this case if it has already > threshold $1 + \\epsilon$, we will not allow A to make it larger by stoping the gradient of A. On the other hand if the ratio is less than the threshold, we will not stop A's gradient because we think the difference between original policy and current policy is still tolerable, we can still augment it.\n",
        "\n",
        "When $A < 0$ we will stop A's gradient when $\\frac{\\pi_\\theta(a_t^n|s_t^n)}{\\pi_{\\theta_{old}}(a_t^n|s_t^n)}$ is less than threshold because otherwise this negative A's gradient will make the ratio even smaller. However when the ratio is still around 1, we do nothing and let the gradient of A (a negetive number) pass by to reduce the ratio.\n",
        "\n",
        "In summury, it turns out that by dynamically stopping gradient when policy becomes more different that we admit, we can achieve the same effect as constraint KL divergence."
      ],
      "metadata": {
        "id": "bdI_NLiAzTdj"
      }
    }
  ]
}